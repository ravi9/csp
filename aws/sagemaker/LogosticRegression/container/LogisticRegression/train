# file: train
#===============================================================================
# Copyright 2014-2018 Intel Corporation.
#
# This software and the related documents are Intel copyrighted  materials,  and
# your use of  them is  governed by the  express license  under which  they were
# provided to you (License).  Unless the License provides otherwise, you may not
# use, modify, copy, publish, distribute,  disclose or transmit this software or
# the related documents without Intel's prior written permission.
#
# This software and the related documents  are provided as  is,  with no express
# or implied  warranties,  other  than those  that are  expressly stated  in the
# License.
#===============================================================================

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
from pprint import pprint, pformat

import pandas as pd
import numpy as np
import time
from LogisticRegression import LogisticRegression

import logging

logging.basicConfig(
    format='%(asctime)s %(levelname)-8s %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S')
   
logger = logging.getLogger(__name__)


# These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')
input_config_path = os.path.join(prefix, 'input/config/inputdataconfig.json')
# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.
def train():
    logger.info('Container setup completed, In Docker entrypoint - train... ')
    
    try:
        # Default Paramaters         
        params = {}
        params['nClasses'] = 0
        params['penaltyL1'] = 0
        params['penaltyL2'] = 0
        params['interceptFlag'] = True
        params['dtype'] = "float"

        logger.info("Default Hyperparameters loaded: ")
        logger.info(pformat(params))

        # Check input data mode and number of channels
        with open(input_config_path, "r") as file:
            input_config = json.load(file)
        channels = list(input_config.keys())
        if len(channels) != 1:
            raise ValueError("This container supports only 1 channel")
        channel = channels[0]
        input_data_mode = input_config[channel]["TrainingInputMode"]
        
        logger.info("Reading training data... ")
        if input_data_mode == "Pipe":
            training_path = prefix + "/input/data/" + channel + "_0"
            train_data = pd.read_csv(training_path, header=None)
        elif input_data_mode == "File":
            training_path = prefix + "/input/data/" + channel + "/"
            files = os.listdir(training_path)
            if len(files) != 0:
                # Take the set of files and read them all into a single pandas dataframe
                input_files = [os.path.join(training_path, file) for file in files]
                raw_data = [pd.read_csv(file, header=None) for file in input_files]
                for file in raw_data:
                    logger.info("shape: " + str(file.shape))
                train_data = pd.concat(raw_data)
            else:
                raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        else:
            raise ValueError("Unknown input data mode")

        logger.info("Training Data Shape: " + str(train_data.shape))
        # Read in any hyperparameters that the user passed with the training job
        solverParams = {}
        solverParams['solverName'] = ""
        solverParams['solverMethod'] = "defaultDense"
        solverParams['solverMaxIterations'] = 1000
        solverParams['solverLearningRate'] = 0.001
        solverParams['solverStepLength'] = 0.001
        solverParams['solverAccuracyThreshold'] = 0.0001
        solverParams['solverBatchSize'] = train_data.shape[0]
        solverParams['solverCorrectionPairBatchSize'] = train_data.shape[0]
        solverParams['solverL'] = 1
                
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
        for param in trainingParams:
            #logger.info("###param: " + str(param))
            if "solver" in param:
                solverParams[param] = trainingParams[param]
                #logger.info("###solverParams[param]: " + str(solverParams[param]))
            else:
                params[param] = trainingParams[param]
        logger.info("Updated with user hyperparameters, Final Hyperparameters: ")
        logger.info(pformat(params))
        if solverParams['solverName'] == "":
            logger.info("Default SGD momentum solver is used as solver parammeters were not specified.")
        else:
            logger.info(pformat(solverParams))
        logger.info("If optional parameters were not specified default values will be used.")
        # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
        # strings, so we need to do any necessary conversions

        # Now use DAAL's Logistic Regression to train the model.
        logger.info('Starting DAAL Logistic Regression training...')
        nClasses = int(params['nClasses'])
        if nClasses == 0:
            raise ValueError("nClasses parameter has to be specified to positive non-zero value")

        logistic_regression = LogisticRegression(
                                                 nClasses=nClasses,
                                                 penaltyL1=float(params['penaltyL1']),
                                                 penaltyL2=float(params['penaltyL2']),
                                                 interceptFlag = params['interceptFlag'],
                                                 dtype = params['dtype'],
                                                 optSolverParam = solverParams
                                                )
        dtype = (np.float64 if params['dtype'] == "double" else np.float32)
        X = np.ascontiguousarray(train_data.values, dtype=dtype)
        train_data = np.ascontiguousarray(X[:,:-1], dtype=dtype)
        labels = np.ascontiguousarray(X[:,-1].reshape(len(X),1), dtype=dtype)
        
        start = time.time()
        logistic_regression.train(train_data=train_data, train_labels=labels)
        end = time.time()
        logger.info("Training time in sec = " + str(end - start))

        result_buffer = np.zeros(2,dtype=np.int)
        result_buffer[0] = nClasses
        result_buffer[1] = train_data.shape[1]
        result_type = params['dtype']
        
        # save the model
        np.savetxt(os.path.join(model_path, 'daal-log-reg-train-features-classes.csv'), result_buffer, delimiter=",")
        text_file = open(os.path.join(model_path, 'daal-log-reg-dtype.txt'), "w")
        text_file.write(result_type)
        text_file.close()
        logger.info('number of classes saved at ' + str(os.path.join(model_path, 'daal-log-reg-train-features-classes.csv')))
        logger.info('dtype saved at ' + str(os.path.join(model_path, 'daal-log-reg-dtype.txt')))
        with open(os.path.join(model_path, 'daal-log-reg-train-model.pkl'), 'wb') as out_path:
            pickle.dump(logistic_regression.trainingResult.model, out_path)
            logger.info('Model saved at ' + str(out_path))

    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def printNT(table, nrows = 0, message=''):
    npa = getArrayFromNT(table, nrows)
    print(message, '\n', npa)

if __name__ == '__main__':
    train()
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
